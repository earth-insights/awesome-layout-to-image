# awesome-layout-to-image

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re/) [![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com/) [![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://https://github.com/earth-insights/awesome-layout-to-image/graphs/commit-activity)

<!-- ![img](https://i.imgur.com/Ky2jxnj.png) -->

## mask to image

|Abbreviation|Type|Title|Publication|Paper|Code|
|:---:|:---:|---|:---:|:---:|:---:|
|**OASIS**|mask->image|**You Only Need Adversarial Supervision for Semantic Image Synthesis**|ICLR'2021|[OASIS](https://arxiv.org/abs/2012.04781)|[link](https://github.com/boschresearch/OASIS)|
|**FreestyleNet**|mask,text->image|**Freestyle Layout-to-Image Synthesis**|CVPR'2023|[FreestyleNet](https://arxiv.org/abs/2303.14412)|[link](https://github.com/essunny310/FreestyleNet)|
|**ControlNet**|mask,(text)->image|**Adding Conditional Control to Text-to-Image Diffusion Models**|ICCV'2023|[ControlNet](https://arxiv.org/abs/2302.05543)|[link](https://github.com/lllyasviel/ControlNet)|
|**Changen**|mask->image|**Scalable Multi-Temporal Remote Sensing Change Data Generation via Simulating Stochastic Change Process**|ICCV'2023|[Changen](https://arxiv.org/abs/2309.17031)|[link](https://github.com/Z-Zheng/Changen)|
|**Dataset Diffusion**|text->image,mask|**Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation**|NIPS'2024|[Dataset Diffusion](https://arxiv.org/abs/2309.14303)|[link](https://github.com/VinAIResearch/Dataset-Diffusion)|
|**FreeMask**|mask,text->image|**FreeMask: Synthetic Images with Dense Annotations Make Stronger Segmentation Models**|NIPS'2024|[FreeMask](https://arxiv.org/abs/2310.15160)|[link](https://github.com/LiheYoung/FreeMask)|
|**Uni-ControlNet**|mask,(text)->image|**Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models**|NIPS'2024|[Uni-ControlNet](https://arxiv.org/abs/2305.16322)|[link](https://github.com/ShihaoZhaoZSH/Uni-ControlNet)|
|**SatSynth**|->image,mask|**SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation**|CVPR'2024|[SatSynth](http://arxiv.org/abs/2403.16605)|/|
|**CRS-Diff**|mask,(text)->image|**CRS-Diff: Controllable Generative Remote Sensing Foundation Modeln**|arXiv'2024|[CRS-Diff](https://arxiv.org/abs/2403.11614)|[link](https://github.com/Sonettoo/CRS-Diff)|
|**ChangeAnywhere**|mask->image|**ChangeAnywhere: Sample Generation for Remote Sensing Change Detection via Semantic Latent Diffusion Model**|arXiv'2024|[ChangeAnywhere](https://arxiv.org/abs/2404.08892)|[link](https://github.com/tangkai-RS/ChangeAnywhere)|

## bbox to image

|Abbreviation|Title|Publication|Paper|Code|
|:---:|---|:---:|:---:|:---:|
|**ReCo**|**ReCo: Region-Controlled Text-to-Image Generation**|CVPR'2023|[ReCo](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.pdf)|[link](https://github.com/microsoft/ReCo)
|**SSMG**|**SSMG: Spatial-Semantic Map Guided Diffusion Model for Free-Form Layout-to-Image Generation**|AAAI'2024|[SSMG](https://arxiv.org/pdf/2308.10156v2.pdf)|
|**LayoutDiff**|**LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation**|CVPR'2023|[LayoutDiff](https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.html)|[link](https://github.com/ZGCTroy/LayoutDiffusion)

## key point to image

|Abbreviation|Title|Publication|Paper|Code|
|:---:|---|:---:|:---:|:---:|


...
